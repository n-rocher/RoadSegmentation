{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm *.npz\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5z5ly8qxVYX",
        "outputId": "f619f32a-49ff-4749-c261-348f2b4296f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.npz': No such file or directory\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.9)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.25)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.2.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up "
      ],
      "metadata": {
        "id": "z5v8OBUF1LcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ibO-B8sxXum",
        "outputId": "1ae7fc16-9056-4236-a646-2a057a3d49d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wiXxTL5dtDwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2b469a-c77b-4d57-c912-aa3217070b13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100-validation_y_0.npz',\n",
              " '100-validation_y_1.npz',\n",
              " '100-validation_y_2.npz',\n",
              " '100-validation_y_3.npz',\n",
              " '100-validation_y_4.npz',\n",
              " '100-validation_y_5.npz',\n",
              " '100-validation_y_6.npz',\n",
              " '100-validation_y_7.npz',\n",
              " '100-validation_y_8.npz',\n",
              " '100-validation_y_9.npz',\n",
              " '100-validation_y_10.npz',\n",
              " '100-validation_y_11.npz',\n",
              " '100-validation_y_12.npz',\n",
              " '100-validation_y_13.npz',\n",
              " '100-validation_y_14.npz',\n",
              " '100-validation_y_15.npz',\n",
              " '100-validation_y_16.npz',\n",
              " '100-validation_y_17.npz',\n",
              " 'colab.ipynb',\n",
              " '1-test_y_0.npz',\n",
              " '100-validation_x_6.npz',\n",
              " '100-validation_x_7.npz',\n",
              " '100-validation_x_9.npz',\n",
              " '100-validation_x_8.npz',\n",
              " '100-validation_x_10.npz',\n",
              " '100-validation_x_12.npz',\n",
              " '100-validation_x_11.npz',\n",
              " '100-validation_x_13.npz',\n",
              " '100-validation_x_15.npz',\n",
              " '100-validation_x_14.npz',\n",
              " '100-validation_x_16.npz',\n",
              " '100-validation_x_0.npz',\n",
              " '1-test_x_0.npz',\n",
              " '100-validation_x_17.npz',\n",
              " '100-validation_x_1.npz',\n",
              " '100-validation_x_3.npz',\n",
              " '100-validation_x_2.npz',\n",
              " '100-validation_x_4.npz',\n",
              " '100-validation_x_5.npz',\n",
              " 'BiSeNet-V2_MultiDataset_512-512_epoch-13_loss-0.23_miou_0.54.h5']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow.keras.losses as losses\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.metrics as metrics\n",
        "\n",
        "import os\n",
        "from tensorflow import argmax\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/\")\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "k0uTa0D5uXO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SanityCheck(keras.callbacks.Callback):\n",
        "\n",
        "    id_batch = -1\n",
        "    epoch = 0\n",
        "\n",
        "    DEBUG = False\n",
        "\n",
        "    def __init__(self, dataset, output=\"./\", regulator=200, export_files=True, export_wandb=True):\n",
        "        super(SanityCheck, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.data = self.dataset.__getitem__(0)\n",
        "        self.image_size = [self.data[0][0].shape[0], self.data[0][0].shape[1]]\n",
        "        self.output = output\n",
        "        self.regulator = regulator\n",
        "        self.export_files = export_files\n",
        "        self.export_wandb = export_wandb\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # if self.id_batch > 25:\n",
        "        self.process_test()\n",
        "\n",
        "        self.id_batch = -1\n",
        "        self.epoch += 1\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        self.id_batch += 1\n",
        "        if self.id_batch > 25 and self.id_batch % self.regulator == 0:\n",
        "            self.process_test()\n",
        "\n",
        "    def predict_mask(self, img, mask):\n",
        "        if self.DEBUG == True:\n",
        "            result = np.repeat(np.expand_dims(np.zeros(self.image_size), axis=2), 2, axis=2)  # Forme du résultat : (X, Y, 2)\n",
        "        else:\n",
        "            result = self.model.predict(np.expand_dims(img, axis=0))[0]\n",
        "\n",
        "        result = np.array(argmax(result, axis=-1), dtype=np.uint8)\n",
        "        mask = np.array(argmax(mask, axis=-1), dtype=np.uint8)\n",
        "\n",
        "        return result, mask\n",
        "\n",
        "    def extract_file(self, result):\n",
        "        os.makedirs(self.output, exist_ok=True)\n",
        "\n",
        "        # plt.rcParams[\"figure.figsize\"] = (14, 20)\n",
        "        fig, axs = plt.subplots(3, len(result))\n",
        "        fig.suptitle((\"MODEL-NAME\" if self.DEBUG else self.model.name) + \" - I M S\")\n",
        "\n",
        "        colors = self.dataset.colors()\n",
        "        for i_img, (img_i, mask_i, seg_i) in enumerate(result):\n",
        "\n",
        "            # Colorisation du masque et du résultat\n",
        "            mask = np.zeros(img_i.shape, dtype=np.uint8)\n",
        "            seg = np.zeros(img_i.shape, dtype=np.uint8)\n",
        "            for categorie in colors.keys():\n",
        "                mask[mask_i == categorie] = colors[categorie][\"color\"]\n",
        "                seg[seg_i == categorie] = colors[categorie][\"color\"]\n",
        "\n",
        "            # Affichages des images\n",
        "\n",
        "            axs[0, i_img].imshow(img_i)\n",
        "            axs[0, i_img].axis('off')\n",
        "\n",
        "            axs[1, i_img].imshow(mask)\n",
        "            axs[1, i_img].axis('off')\n",
        "\n",
        "            axs[2, i_img].imshow(seg)\n",
        "            axs[2, i_img].axis('off')\n",
        "\n",
        "        plt.subplots_adjust(wspace=.05, hspace=.05)\n",
        "        fig.savefig(\"%s/%d_%d.png\" % (self.output, self.epoch, self.id_batch), dpi=1000, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def extract_wandb(self, result):\n",
        "        labels = self.dataset.labels()\n",
        "        wandb_mask_list = list(map(lambda x: wandb.Image(x[0], masks={\"prediction\": {\"mask_data\": x[2], \"class_labels\": labels}, \"ground truth\": {\"mask_data\": x[1], \"class_labels\": labels}}), result))\n",
        "        wandb.log({\"Predictions\" : wandb_mask_list})\n",
        "\n",
        "    def process_test(self):\n",
        "\n",
        "        result=[]\n",
        "        imgs, masks=self.data\n",
        "\n",
        "        for img_i, mask_i in zip(imgs, masks):\n",
        "            seg, mask_i=self.predict_mask(img_i, mask_i)\n",
        "            result.append((img_i, mask_i, seg))\n",
        "\n",
        "        if self.export_files:\n",
        "            self.extract_file(result)\n",
        "\n",
        "        if self.export_wandb:\n",
        "            self.extract_wandb(result)"
      ],
      "metadata": {
        "id": "iCGbrqEbuHuy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArgmaxMeanIOU(metrics.MeanIoU):\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        return super().update_state(tf.argmax(y_true, axis=-1), tf.argmax(y_pred, axis=-1), sample_weight)"
      ],
      "metadata": {
        "id": "DJNfVHr_uWd6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "tCcFhRu0uivx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.losses as losses\n",
        "import tensorflow.keras.metrics as metrics\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "import keras.utils.conv_utils as conv_utils\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "# default input shape\n",
        "INPUT_SHAPE = (512, 1024, 3)\n",
        "\n",
        "def normalize_data_format(value):\n",
        "    if value is None:\n",
        "        value = K.image_data_format()\n",
        "    data_format = value.lower()\n",
        "    if data_format not in {'channels_first', 'channels_last'}:\n",
        "        raise ValueError('The `data_format` argument must be one of '\n",
        "                         '\"channels_first\", \"channels_last\". Received: ' +\n",
        "                         str(value))\n",
        "    return data_format\n",
        "\n",
        "class BilinearUpSampling2D(Layer):\n",
        "    def __init__(self, size=(2, 2), data_format=None, **kwargs):\n",
        "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
        "        self.data_format = normalize_data_format(data_format)\n",
        "        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n",
        "        self.input_spec = InputSpec(ndim=4)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n",
        "            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n",
        "            return (input_shape[0],\n",
        "                    input_shape[1],\n",
        "                    height,\n",
        "                    width)\n",
        "        elif self.data_format == 'channels_last':\n",
        "            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
        "            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
        "            return (input_shape[0],\n",
        "                    height,\n",
        "                    width,\n",
        "                    input_shape[3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = K.shape(inputs)\n",
        "        if self.data_format == 'channels_first':\n",
        "            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n",
        "            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n",
        "        elif self.data_format == 'channels_last':\n",
        "            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
        "            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
        "        \n",
        "        return tf.image.resize(inputs, [height, width], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'size': self.size, 'data_format': self.data_format}\n",
        "        base_config = super(BilinearUpSampling2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def ge_layer(x_in, c, e=6, stride=1):\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(3, 3), padding='same')(x_in)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    if stride == 2:\n",
        "        x = layers.DepthwiseConv2D(depth_multiplier=e, kernel_size=(3, 3), strides=2, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        y = layers.DepthwiseConv2D(depth_multiplier=e, kernel_size=(3, 3), strides=2, padding='same')(x_in)\n",
        "        y = layers.BatchNormalization()(y)\n",
        "        y = layers.Conv2D(filters=c, kernel_size=(1, 1), padding='same')(y)\n",
        "        y = layers.BatchNormalization()(y)\n",
        "    else:\n",
        "        y = x_in\n",
        "\n",
        "    x = layers.DepthwiseConv2D(depth_multiplier=e, kernel_size=(3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(1, 1), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Add()([x, y])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def stem(x_in, c):\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(3, 3), strides=2, padding='same')(x_in)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x_split = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=c // 2, kernel_size=(1, 1), padding='same')(x_split)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(3, 3), strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    y = layers.MaxPooling2D()(x_split)\n",
        "\n",
        "    x = layers.Concatenate()([x, y])\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def detail_conv2d(x_in, c, stride=1):\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(3, 3), strides=stride, padding='same')(x_in)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def context_embedding(x_in, c):\n",
        "    x = layers.GlobalAveragePooling2D()(x_in)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Reshape((1, 1, c))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(1, 1), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    # broadcasting no needed\n",
        "\n",
        "    x = layers.Add()([x, x_in])\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(3, 3), padding='same')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def bilateral_guided_aggregation(detail, semantic, c):\n",
        "    # detail branch\n",
        "    detail_a = layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same')(detail)\n",
        "    detail_a = layers.BatchNormalization()(detail_a)\n",
        "\n",
        "    detail_a = layers.Conv2D(filters=c, kernel_size=(1, 1), padding='same')(detail_a)\n",
        "\n",
        "    detail_b = layers.Conv2D(filters=c, kernel_size=(3, 3), strides=2, padding='same')(detail)\n",
        "    detail_b = layers.BatchNormalization()(detail_b)\n",
        "\n",
        "    detail_b = layers.AveragePooling2D((3, 3), strides=2, padding='same')(detail_b)\n",
        "\n",
        "    # semantic branch\n",
        "    semantic_a = layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same')(semantic)\n",
        "    semantic_a = layers.BatchNormalization()(semantic_a)\n",
        "\n",
        "    semantic_a = layers.Conv2D(filters=c, kernel_size=(1, 1), padding='same')(semantic_a)\n",
        "    semantic_a = layers.Activation('sigmoid')(semantic_a)\n",
        "\n",
        "    semantic_b = layers.Conv2D(filters=c, kernel_size=(3, 3), padding='same')(semantic)\n",
        "    semantic_b = layers.BatchNormalization()(semantic_b)\n",
        "\n",
        "    semantic_b = layers.UpSampling2D((4, 4), interpolation='bilinear')(semantic_b)\n",
        "    semantic_b = layers.Activation('sigmoid')(semantic_b)\n",
        "\n",
        "    # combining\n",
        "    detail = layers.Multiply()([detail_a, semantic_b])\n",
        "    semantic = layers.Multiply()([semantic_a, detail_b])\n",
        "\n",
        "    # this layer is not mentioned in the paper !?\n",
        "    #semantic = layers.UpSampling2D((4,4))(semantic)\n",
        "    semantic = layers.UpSampling2D((4, 4), interpolation='bilinear')(semantic)\n",
        "\n",
        "    x = layers.Add()([detail, semantic])\n",
        "    x = layers.Conv2D(filters=c, kernel_size=(3, 3), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def seg_head(x_in, c_t, out_scale, num_classes):\n",
        "    x = layers.Conv2D(filters=c_t, kernel_size=(3, 3), padding='same')(x_in)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=num_classes, kernel_size=(3, 3), padding='same')(x)\n",
        "    x = layers.UpSampling2D((out_scale, out_scale), interpolation='bilinear')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def BiSeNetV2(num_classes=2, out_scale=8, input_shape=INPUT_SHAPE, l=4, seghead_expand_ratio=2):\n",
        "    x_in = layers.Input(input_shape)\n",
        "\n",
        "    # semantic branch\n",
        "    # S1 + S2\n",
        "    x = stem(x_in, 64 // l)\n",
        "\n",
        "    # S3\n",
        "    x = ge_layer(x, 128 // l, stride=2)\n",
        "    x = ge_layer(x, 128 // l, stride=1)\n",
        "\n",
        "    # S4\n",
        "    x = ge_layer(x, 64, stride=2)\n",
        "    x = ge_layer(x, 64, stride=1)\n",
        "\n",
        "    # S5\n",
        "    x = ge_layer(x, 128, stride=2)\n",
        "\n",
        "    x = ge_layer(x, 128, stride=1)\n",
        "    x = ge_layer(x, 128, stride=1)\n",
        "    x = ge_layer(x, 128, stride=1)\n",
        "\n",
        "    x = context_embedding(x, 128)\n",
        "\n",
        "    # detail branch\n",
        "    # S1\n",
        "    y = detail_conv2d(x_in, 64, stride=2)\n",
        "    y = detail_conv2d(y, 64, stride=1)\n",
        "\n",
        "    # S2\n",
        "    y = detail_conv2d(y, 64, stride=2)\n",
        "    y = detail_conv2d(y, 64, stride=1)\n",
        "    y = detail_conv2d(y, 64, stride=1)\n",
        "\n",
        "    # S3\n",
        "    y = detail_conv2d(y, 128, stride=2)\n",
        "    y = detail_conv2d(y, 128, stride=1)\n",
        "    y = detail_conv2d(y, 128, stride=1)\n",
        "\n",
        "    x = bilateral_guided_aggregation(y, x, 128)\n",
        "\n",
        "    x = seg_head(x, num_classes * seghead_expand_ratio, out_scale, num_classes)\n",
        "\n",
        "    model = models.Model(inputs=[x_in], outputs=[x], name=\"BiSeNet-V2\")\n",
        "\n",
        "    # set weight initializers\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel_initializer'):\n",
        "            layer.kernel_initializer = tf.keras.initializers.HeNormal()\n",
        "        if hasattr(layer, 'depthwise_initializer'):\n",
        "            layer.depthwise_initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    return model\n",
        "\n",
        "def bisenetv2_DEEPER(num_classes=2, out_scale=8, input_shape=INPUT_SHAPE, l=4, seghead_expand_ratio=2):\n",
        "    x_in = layers.Input(input_shape)\n",
        "\n",
        "    # semantic branch\n",
        "    # S1 + S2\n",
        "    x = stem(x_in, 64 // l)\n",
        "\n",
        "    # S3\n",
        "    x = ge_layer(x, 128 // l, stride=2)\n",
        "    x = ge_layer(x, 128 // l, stride=1)\n",
        "    \n",
        "\n",
        "    # S3 ++ \n",
        "    x = ge_layer(x, 256 // l, stride=2)\n",
        "    x = ge_layer(x, 256 // l, stride=1)\n",
        "\n",
        "    # S4\n",
        "    x = ge_layer(x, 64, stride=2)\n",
        "    x = ge_layer(x, 64, stride=1)\n",
        "\n",
        "    # S5\n",
        "    x = ge_layer(x, 128, stride=2)\n",
        "\n",
        "    x = ge_layer(x, 128, stride=1)\n",
        "    x = ge_layer(x, 128, stride=1)\n",
        "    x = ge_layer(x, 128, stride=1)\n",
        "\n",
        "    print(x.shape)\n",
        "\n",
        "    x = context_embedding(x, 128)\n",
        "\n",
        "    # detail branch\n",
        "    # S1\n",
        "    y = detail_conv2d(x_in, 64, stride=2)\n",
        "    y = detail_conv2d(y, 64, stride=1)\n",
        "\n",
        "    # S2\n",
        "    y = detail_conv2d(y, 64, stride=2)\n",
        "    y = detail_conv2d(y, 64, stride=1)\n",
        "    y = detail_conv2d(y, 64, stride=1)\n",
        "\n",
        "    # S3\n",
        "    y = detail_conv2d(y, 128, stride=2)\n",
        "    y = detail_conv2d(y, 128, stride=1)\n",
        "    y = detail_conv2d(y, 128, stride=1)\n",
        "\n",
        "    # S3 ++\n",
        "    y = detail_conv2d(y, 256, stride=2)\n",
        "    y = detail_conv2d(y, 256, stride=1)\n",
        "    y = detail_conv2d(y, 256, stride=1)\n",
        "\n",
        "    x = bilateral_guided_aggregation(y, x, 256) # AVANT 128 \n",
        "\n",
        "    x = seg_head(x, num_classes * seghead_expand_ratio, out_scale, num_classes)\n",
        "\n",
        "    model = models.Model(inputs=[x_in], outputs=[x], name=\"BiSeNet-V2-Deeper\")\n",
        "\n",
        "    # set weight initializers\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel_initializer'):\n",
        "            layer.kernel_initializer = tf.keras.initializers.HeNormal()\n",
        "        if hasattr(layer, 'depthwise_initializer'):\n",
        "            layer.depthwise_initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QoO99GsnukKW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "3xdujCYZuaOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l_9l70EYtDwm"
      },
      "outputs": [],
      "source": [
        "DATASET_FILE_SIZE = 100\n",
        "\n",
        "USE_WANDB = True\n",
        "\n",
        "IMG_SIZE = (512, 512)\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 30\n",
        "LR = 1e-4\n",
        "\n",
        "\n",
        "MAPILLARY_VISTAS_CATEGORIES = {\n",
        "    1: {\"name\": \"Road\", \"color\": [[128, 64, 128], [110, 110, 110]]},\n",
        "    2: {\"name\": \"Lane\", \"color\": [[255, 255, 255], [250, 170, 29], [250, 170, 28], [250, 170, 26], [250, 170, 16], [250, 170, 15], [250, 170, 11], [250, 170, 12], [250, 170, 18], [250, 170, 19], [250, 170, 25], [250, 170, 20], [250, 170, 21], [250, 170, 22], [250, 170, 24]]},\n",
        "    3: {\"name\": \"Crosswalk\", \"color\": [[140, 140, 200], [200, 128, 128]]},\n",
        "    4: {\"name\": \"Curb\", \"color\": [[196, 196, 196], [90, 120, 150]]},\n",
        "    5: {\"name\": \"Sidewalk\", \"color\": [[244, 35, 232]]},\n",
        "\n",
        "    6: {\"name\": \"Traffic Light\", \"color\": [[250, 170, 30]]},\n",
        "    7: {\"name\": \"Traffic Sign\", \"color\": [[220, 220, 0]]},\n",
        "\n",
        "    8: {\"name\": \"Person\", \"color\": [[220, 20, 60]]},\n",
        "\n",
        "    9: {\"name\": \"Bicycle\", \"color\": [[119, 11, 32], [255, 0, 0]]},\n",
        "    10: {\"name\": \"Bus\", \"color\": [[0, 60, 100]]},\n",
        "    11: {\"name\": \"Car\", \"color\": [[0, 0, 142], [0, 0, 90], [0, 0, 110]]},\n",
        "    12: {\"name\": \"Motorcycle\", \"color\": [[0, 0, 230], [255, 0, 200], [255, 0, 100]]},\n",
        "    13: {\"name\": \"Truck\", \"color\": [[0, 0, 70]]},\n",
        "    \n",
        "    14: {\"name\": \"Sky\", \"color\": [[70, 130, 180]]},\n",
        "    15: {\"name\": \"Nature\", \"color\": [[107, 142, 35], [152, 251, 152]]}\n",
        "}\n",
        "\n",
        "AUDI_A2D2_CATEGORIES = {\n",
        "    1: {\"name\": \"Road\", \"color\": [[180, 50, 180], [255, 0, 255]]},\n",
        "    2: {\"name\": \"Lane\", \"color\": [[255, 193, 37], [200, 125, 210], [128, 0, 255]]},\n",
        "    3: {\"name\": \"Crosswalk\", \"color\": [[210, 50, 115]]},\n",
        "    4: {\"name\": \"Curb\", \"color\": [[128, 128, 0]]},\n",
        "    5: {\"name\": \"Sidewalk\", \"color\": [[180, 150, 200]]},\n",
        "\n",
        "    6: {\"name\": \"Traffic Light\", \"color\": [[0, 128, 255], [30, 28, 158], [60, 28, 100]]},\n",
        "    7: {\"name\": \"Traffic Sign\", \"color\": [[0, 255, 255], [30, 220, 220], [60, 157, 199]]},\n",
        "\n",
        "    8: {\"name\": \"Person\", \"color\": [[204, 153, 255], [189, 73, 155], [239, 89, 191]]},\n",
        "\n",
        "    9: {\"name\": \"Bicycle\", \"color\": [[182, 89, 6], [150, 50, 4], [90, 30, 1], [90, 30, 30]]},\n",
        "    10: {\"name\": \"Bus\", \"color\": []},\n",
        "    11: {\"name\": \"Car\", \"color\": [[255, 0, 0], [200, 0, 0], [150, 0, 0], [128, 0, 0]]},\n",
        "    12: {\"name\": \"Motorcycle\", \"color\": [[0, 255, 0], [0, 200, 0], [0, 150, 0]]},\n",
        "    13: {\"name\": \"Truck\", \"color\": [[255, 128, 0], [200, 128, 0], [150, 128, 0], [255, 255, 0], [255, 255, 200]]},\n",
        "\n",
        "    14: {\"name\": \"Sky\", \"color\": [[135, 206, 255]]},\n",
        "    15: {\"name\": \"Nature\", \"color\": [[147, 253, 194]]}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QrP8AKPrtDwn"
      },
      "outputs": [],
      "source": [
        "class NPZDataset(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, dataset_type, length):\n",
        "\n",
        "        self.files_name = self.get_files(dataset_type)\n",
        "        self.current_batch = None\n",
        "        self.current_batch_id = None\n",
        "        self.length = length\n",
        "        \n",
        "\n",
        "    def get_files(self, dataset_type):\n",
        "        data = list(sorted(glob.glob('*-' + dataset_type + '_x_*.npz')))\n",
        "\n",
        "        print(\"Nom du dataset avec size:\", list(map(lambda x: x[0:x.index(\"-\")], data)))\n",
        "        return data\n",
        "\n",
        "    def classes(self):\n",
        "        return len(AUDI_A2D2_CATEGORIES) + 1\n",
        "\n",
        "    def labels(self):\n",
        "        l = {0: \"Background\"}\n",
        "        for i, label in enumerate(map(lambda x: AUDI_A2D2_CATEGORIES[x][\"name\"], AUDI_A2D2_CATEGORIES), start=1):\n",
        "            l[i] = label\n",
        "        return l\n",
        "\n",
        "    def name(self):\n",
        "        return \"NPZDataset\"\n",
        "\n",
        "    def __len__(self):\n",
        "        INDEX_DEBUT = 0\n",
        "        return sum(list(map(lambda x: int(x[INDEX_DEBUT:x.index(\"-\")]), self.files_name)))\n",
        "\n",
        "    def __getitem__(self, batch_id):\n",
        "\n",
        "        index_file = batch_id // self.length\n",
        "        batch_number = batch_id % self.length\n",
        "\n",
        "        if self.current_batch_id != index_file:\n",
        "            self.current_batch_id = index_file\n",
        "            print(\"---\", batch_id, \"-> Changing the batch file\")\n",
        "            self.current_batch = None\n",
        "            with np.load(self.files_name[index_file]) as data_x:\n",
        "                with np.load(self.files_name[index_file].replace(\"x\", \"y\")) as data_y:\n",
        "                    self.current_batch = {\n",
        "                        \"x\" : data_x[\"arr_0\"],\n",
        "                        \"y\": data_y[\"arr_0\"]\n",
        "                    }\n",
        "\n",
        "        x = self.current_batch[\"x\"][batch_number]\n",
        "        y = self.current_batch[\"y\"][batch_number]\n",
        "        \n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RjtfiW_A_vzQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1TPMOB_-_-co"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f7dQyMGZANGQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RE3pjEdYAbvq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = NPZDataset('validation', 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyRmlyZWyV8l",
        "outputId": "b1ef240e-3983-4dea-d6d6-3429c28da4ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nom du dataset avec size: ['100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzBHg0gEtDwo",
        "outputId": "02cd7d97-ce2a-4ab1-871b-936bb27ebaeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13249834530450959027\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11320098816\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 16388493014480480952\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nUKkvB62tDwp",
        "outputId": "93ac10b1-1319-4943-c8b9-fe8505d94501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nom du dataset avec size: ['100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100']\n",
            "Nom du dataset avec size: ['100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100', '100']\n",
            "Nom du dataset avec size: ['1']\n",
            "train_dataset : 1800 batchs - 7200 images\n",
            "validation_dataset : 1800 batchs - 7200 images\n",
            "test_dataset : 1 batchs - 4 images\n",
            "\n",
            "> Creating model\n",
            "--- 0 -> Changing the batch file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnrocher\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/nrocher/Road%20Segmentation/runs/3jvy025n\" target=\"_blank\">peach-star-18</a></strong> to <a href=\"https://wandb.ai/nrocher/Road%20Segmentation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> Training\n",
            "--- 0 -> Changing the batch file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  99/1800 [>.............................] - ETA: 12:19 - loss: 0.2262 - accuracy: 0.9197 - argmax_mean_iou: 0.5218--- 100 -> Changing the batch file\n",
            " 199/1800 [==>...........................] - ETA: 13:04 - loss: 0.2191 - accuracy: 0.9229 - argmax_mean_iou: 0.5185--- 200 -> Changing the batch file\n",
            " 299/1800 [===>..........................] - ETA: 12:40 - loss: 0.2123 - accuracy: 0.9254 - argmax_mean_iou: 0.5202--- 300 -> Changing the batch file\n",
            " 399/1800 [=====>........................] - ETA: 12:04 - loss: 0.2104 - accuracy: 0.9261 - argmax_mean_iou: 0.5304--- 400 -> Changing the batch file\n",
            " 499/1800 [=======>......................] - ETA: 11:19 - loss: 0.2050 - accuracy: 0.9282 - argmax_mean_iou: 0.5315--- 500 -> Changing the batch file\n",
            " 599/1800 [========>.....................] - ETA: 10:40 - loss: 0.2029 - accuracy: 0.9292 - argmax_mean_iou: 0.5381--- 600 -> Changing the batch file\n",
            " 699/1800 [==========>...................] - ETA: 9:53 - loss: 0.2024 - accuracy: 0.9295 - argmax_mean_iou: 0.5392--- 700 -> Changing the batch file\n",
            " 799/1800 [============>.................] - ETA: 9:05 - loss: 0.2012 - accuracy: 0.9299 - argmax_mean_iou: 0.5378--- 800 -> Changing the batch file\n",
            " 899/1800 [=============>................] - ETA: 8:12 - loss: 0.2008 - accuracy: 0.9299 - argmax_mean_iou: 0.5379--- 900 -> Changing the batch file\n",
            " 999/1800 [===============>..............] - ETA: 7:19 - loss: 0.2020 - accuracy: 0.9293 - argmax_mean_iou: 0.5347--- 1000 -> Changing the batch file\n",
            "1099/1800 [=================>............] - ETA: 6:27 - loss: 0.1995 - accuracy: 0.9303 - argmax_mean_iou: 0.5371--- 1100 -> Changing the batch file\n",
            "1199/1800 [==================>...........] - ETA: 5:32 - loss: 0.1983 - accuracy: 0.9308 - argmax_mean_iou: 0.5369--- 1200 -> Changing the batch file\n",
            "1299/1800 [====================>.........] - ETA: 4:37 - loss: 0.1977 - accuracy: 0.9311 - argmax_mean_iou: 0.5397--- 1300 -> Changing the batch file\n",
            "1399/1800 [======================>.......] - ETA: 3:42 - loss: 0.1962 - accuracy: 0.9317 - argmax_mean_iou: 0.5432--- 1400 -> Changing the batch file\n",
            "1499/1800 [=======================>......] - ETA: 2:47 - loss: 0.1951 - accuracy: 0.9320 - argmax_mean_iou: 0.5459--- 1500 -> Changing the batch file\n",
            "1599/1800 [=========================>....] - ETA: 1:52 - loss: 0.1944 - accuracy: 0.9322 - argmax_mean_iou: 0.5468--- 1600 -> Changing the batch file\n",
            "1699/1800 [===========================>..] - ETA: 56s - loss: 0.1935 - accuracy: 0.9325 - argmax_mean_iou: 0.5482--- 1700 -> Changing the batch file\n",
            "1800/1800 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9324 - argmax_mean_iou: 0.5471--- 0 -> Changing the batch file\n",
            "--- 100 -> Changing the batch file\n",
            "--- 200 -> Changing the batch file\n",
            "--- 300 -> Changing the batch file\n",
            "--- 400 -> Changing the batch file\n",
            "--- 500 -> Changing the batch file\n",
            "--- 600 -> Changing the batch file\n",
            "--- 700 -> Changing the batch file\n",
            "--- 800 -> Changing the batch file\n",
            "--- 900 -> Changing the batch file\n",
            "--- 1000 -> Changing the batch file\n",
            "--- 1100 -> Changing the batch file\n",
            "--- 1200 -> Changing the batch file\n",
            "--- 1300 -> Changing the batch file\n",
            "--- 1400 -> Changing the batch file\n",
            "--- 1500 -> Changing the batch file\n",
            "--- 1600 -> Changing the batch file\n",
            "--- 1700 -> Changing the batch file\n",
            "1800/1800 [==============================] - 1484s 819ms/step - loss: 0.1935 - accuracy: 0.9324 - argmax_mean_iou: 0.5471 - val_loss: 0.1708 - val_accuracy: 0.9388 - val_argmax_mean_iou: 0.5873\n",
            "Epoch 2/30\n",
            "--- 0 -> Changing the batch file\n",
            "  99/1800 [>.............................] - ETA: 12:17 - loss: 0.1651 - accuracy: 0.9405 - argmax_mean_iou: 0.5776--- 100 -> Changing the batch file\n",
            " 199/1800 [==>...........................] - ETA: 13:11 - loss: 0.1675 - accuracy: 0.9398 - argmax_mean_iou: 0.5736--- 200 -> Changing the batch file\n",
            " 252/1800 [===>..........................] - ETA: 13:34 - loss: 0.1646 - accuracy: 0.9408 - argmax_mean_iou: 0.5792"
          ]
        }
      ],
      "source": [
        "train_dataset = NPZDataset('validation', 100)\n",
        "validation_dataset = NPZDataset('validation', 100)\n",
        "test_dataset = NPZDataset('test', 1)\n",
        "\n",
        "\n",
        "print(\"train_dataset :\", len(train_dataset), \"batchs -\", len(train_dataset) * BATCH_SIZE, \"images\")\n",
        "print(\"validation_dataset :\", len(validation_dataset), \"batchs -\", len(validation_dataset) * BATCH_SIZE, \"images\")\n",
        "print(\"test_dataset :\", len(test_dataset), \"batchs -\", len(test_dataset) * BATCH_SIZE, \"images\")\n",
        "\n",
        "\n",
        "# Creating model\n",
        "print(\"\\n> Creating model\")\n",
        "model = BiSeNetV2(num_classes=validation_dataset.classes(), input_shape=IMG_SIZE + (3,))\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=LR)\n",
        "cce = losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer, loss=cce, metrics=['accuracy', ArgmaxMeanIOU(validation_dataset.classes())])\n",
        "\n",
        "\n",
        "model = keras.models.load_model(\"BiSeNet-V2_MultiDataset_512-512_epoch-13_loss-0.23_miou_0.54.h5\", custom_objects={'ArgmaxMeanIOU': ArgmaxMeanIOU})\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "callbacks = [\n",
        "    SanityCheck(test_dataset, output=\"trained_models/\" + now_str + \"/check/\", regulator=500, export_files=False, export_wandb=USE_WANDB),\n",
        "    keras.callbacks.ModelCheckpoint(\"trained_models/\" + now_str + \"/\" + model.name + \"_\" + test_dataset.name() + \"_\" + str(IMG_SIZE[0]) + \"-\" + str(IMG_SIZE[1]) + \"_epoch-{epoch:02d}_loss-{val_loss:.2f}_miou_{val_argmax_mean_iou:.2f}.h5\"),\n",
        "    keras.callbacks.TensorBoard(log_dir=\"trained_models/\" + now_str + \"/logs/\", histogram_freq=1)\n",
        "]\n",
        "\n",
        "if USE_WANDB:\n",
        "    run = wandb.init(project=\"Road Segmentation\", entity=\"nrocher\", config={\n",
        "        \"learning_rate\": LR,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"image_size\": IMG_SIZE,\n",
        "        \"dataset\": test_dataset.name(),\n",
        "        \"model\": model.name\n",
        "    })\n",
        "    callbacks.append(WandbCallback())\n",
        "\n",
        "\n",
        "\n",
        "# Training\n",
        "print(\"\\n> Training\")\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_dataset,\n",
        "    shuffle=False,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Weights & Biases - END\n",
        "if USE_WANDB:\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qEYmrrXJJlr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YlOhgPycJ0VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GhFok3L4KC-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hUJwBJbmKRoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mg6j56HTKgRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ze91XLWRKu7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lNseM_VLJXCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "29cDG05hHTMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nglkdj65Hh2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6qaA0NV8Hwfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5kvKYGdsH_JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "79o7zkAuIN2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tLxVYKMNIcgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C9L0p_qdIrJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1kZzC6-wI5vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d8YQKqGZJIYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I1TZeYQPEU1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JVRcxCYmEjas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3-MsvrYhEyEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P0gWtqlaFAty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fKwl_oHSFPXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sfFjJkVJFeA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nkkhMpb9FsqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "03RZQOTNF7Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "roYLQNDhGJ9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7AX2RGKXGYmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cyqg8ozOGnQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UVQJGNrjG15p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_LYTTQSjHEo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t0W4nTNIBzor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GsTi_4FyCCSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qwl0ptNRCQ7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v0TJq9ydCflJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MWk0erAoCuOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cj4lkLWsC84J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b_Ud0TqEDLhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gt-9TOI7DaLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_l9nvBcBDo02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xsuWIz1TD3eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GlgwP-AqEGHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y6ztYkXrAqZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TSYzDQzlA5Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cMRTkmc3BHsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bKgoJOKWBWZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rpq8H59sBlDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uBF5tJGV_SgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dF9-m3-m_hJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ex0ij2jr-1Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e5qqVRYV-mn1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z5v8OBUF1LcB",
        "k0uTa0D5uXO_",
        "tCcFhRu0uivx"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}